{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import pymongo\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cazton_ivf\n"
     ]
    }
   ],
   "source": [
    "# specify the name of the .env file name \n",
    "\n",
    "\n",
    "env_name = \".env\" # following example.env template change to your own .env file name\n",
    "\n",
    "config = dotenv_values(env_name)\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(env_name, override=True)\n",
    "\n",
    "cosmos_conn = config['cosmos_connection_string']\n",
    "cosmos_database = config['cosmos_database_name']\n",
    "cosmos_collection = config['cosmos_collection_name']\n",
    "cosmos_vector_property = config['cosmos_vector_property_name']\n",
    "cosmos_cache = config['cosmos_cache_collection_name']\n",
    "# Create the Azure Cosmos DB for MongoDB client\n",
    "cosmos_client = pymongo.MongoClient(cosmos_conn)\n",
    "\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_version = config['openai_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_model = config['openai_embeddings_model']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "openai_completions_model = config['openai_completions_model']\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_version)\n",
    "\n",
    "print(cosmos_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database\n",
    "database = cosmos_client[cosmos_database]\n",
    "\n",
    "# Get the movie collection\n",
    "movies = database[cosmos_collection]\n",
    "\n",
    "# Get the cache collection\n",
    "cache = database[cosmos_cache]\n",
    "cache \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=200), stop=stop_after_attempt(20))\n",
    "def generate_embeddings(input_string):\n",
    "    \"\"\"\n",
    "    Retrieves embeddings for the given input string using Azure OpenAI.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string for which embeddings need to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the response from Azure OpenAI embeddings API.\n",
    "    \"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=openai_endpoint,\n",
    "        azure_deployment=openai_embeddings_deployment,\n",
    "        api_version=openai_version,\n",
    "        api_key=openai_key\n",
    "    )\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=input_string,\n",
    "        model=openai_embeddings_model\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# For testing purposes only\n",
    "# test = \"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\"\n",
    "# vectorArray = generate_embeddings(test)\n",
    "# vectorArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(collection, vectors, similarity_score=0.02, num_results=5):\n",
    "    \n",
    "    pipeline = [\n",
    "        {\n",
    "        '$search': {\n",
    "            \"cosmosSearch\": {\n",
    "                \"vector\": vectors,\n",
    "                \"path\": cosmos_vector_property,\n",
    "                \"k\": num_results,\n",
    "                \"efsearch\": 40 # optional for HNSW only \n",
    "            },\n",
    "            \"returnStoredSource\": True }},\n",
    "            { '$project': { 'similarityScore': { '$meta': 'searchScore' }, 'document' : '$$ROOT' } },\n",
    "            { '$match': { \"similarityScore\": { '$gt': similarity_score } } \n",
    "        }   \n",
    "    ]\n",
    "\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "\n",
    "    # Exclude the 'vector' to reduce payload size to LLM and _id properties to avoid serialization issues \n",
    "    for result in results:\n",
    "        del result['document']['vector']\n",
    "        del result['_id']\n",
    "        del result['document']['_id']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab chat history to as part of the payload to GPT model for completion.\n",
    "def get_chat_history(completions=3):\n",
    "\n",
    "    # Sort by _id in descending order and limit the results to the completions value passed in\n",
    "    results = cache.find({}, {\"prompt\": 1, \"completion\": 1}).sort([(\"_id\", -1)]).limit(completions)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(user_prompt, vector_search_results, chat_history):\n",
    "    \n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for the Movie Lens Expert AI Assistant.\n",
    "    You are designed to provide helpful answers to user questions about movies in your database.\n",
    "    You are friendly, helpful, and informative.\n",
    "        - Only answer questions related to the information provided below.\n",
    "        - Write two lines of whitespace between each answer in the list.\n",
    "        - If you're unsure of an answer, you can say \"\"I don't know\"\" or \"\"I'm not sure\"\" and recommend users search themselves.\"\n",
    "    '''\n",
    "\n",
    "    # Create a list of messages as a payload to send to the OpenAI Completions API\n",
    "\n",
    "    # system prompt\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "\n",
    "    #chat history\n",
    "    for chat in chat_history:\n",
    "        messages.append({'role': 'user', 'content': chat['prompt'] + \" \" + chat['completion']})\n",
    "    \n",
    "    #user prompt\n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "\n",
    "    #vector search results\n",
    "    for result in vector_search_results:\n",
    "        messages.append({'role': 'system', 'content': json.dumps(result['document'])})\n",
    "\n",
    "    # Create the completion\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model = openai_completions_deployment,\n",
    "            messages = messages \n",
    "        )\n",
    "    \n",
    "    \n",
    "    return response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_response(user_prompt, prompt_vectors, response):\n",
    "\n",
    "    chat = [\n",
    "        {\n",
    "            'prompt': user_prompt,\n",
    "            'completion': response['choices'][0]['message']['content'],\n",
    "            'completionTokens': str(response['usage']['completion_tokens']),\n",
    "            'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "            'totalTokens': str(response['usage']['total_tokens']),\n",
    "            'model': response['model'],\n",
    "            cosmos_vector_property: prompt_vectors\n",
    "         }\n",
    "    ]\n",
    "\n",
    "    cache.insert_one(chat[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(user_input):\n",
    "\n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    # Similarity score set to 0.99, will only return exact matches. Limit to 1 result.\n",
    "    cache_results = vector_search(cache, user_embeddings, similarity_score=0.02, num_results=5)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "        \n",
    "        return cache_results[0]['document']['completion']\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #perform vector search on the movie collection\n",
    "        search_results = vector_search(movies, user_embeddings)\n",
    "\n",
    "        #chat history\n",
    "        chat_history = get_chat_history(3)\n",
    "\n",
    "        #generate the completion\n",
    "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "        #cache the response\n",
    "        cache_response(user_input, user_embeddings, completions_results)\n",
    "\n",
    "        # Return the generated LLM completion\n",
    "        return completions_results['choices'][0]['message']['content'] \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Ask me anything about movies!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "\n",
    "        # Create a timer to measure the time it takes to complete the request\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get LLM completion\n",
    "        response_payload = chat_completion(user_message)\n",
    "\n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "\n",
    "        elapsed_time = round((end_time - start_time) * 1000, 2)\n",
    "\n",
    "        response = response_payload\n",
    "        \n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append([user_message, response_payload + f\"\\n (Time: {elapsed_time}ms)\"])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    \n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\gradio\\routes.py\", line 695, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1296, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\cwkam\\development\\MoviesRAG\\env\\Lib\\site-packages\\gradio\\utils.py\", line 751, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\cwkam\\AppData\\Local\\Temp\\ipykernel_64604\\2145749619.py\", line 13, in user\n",
      "    response_payload = chat_completion(user_message)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\cwkam\\AppData\\Local\\Temp\\ipykernel_64604\\2863509679.py\", line 8, in chat_completion\n",
      "    cache_results = vector_search(cache, user_embeddings, similarity_score=0.02, num_results=5)\n",
      "                                  ^^^^^\n",
      "NameError: name 'cache' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "demo.close()\n",
    "\n",
    "# launch the gradio interface\n",
    "demo.launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure to run this cell to close or restart the gradio demo\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
